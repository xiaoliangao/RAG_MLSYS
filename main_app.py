# main_app.py
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
import os
import uuid
import shutil
import hashlib
from typing import List, Optional, Dict, Any
from quiz_module import question_generator, evaluator, report_generator
from datetime import datetime
import altair as alt
import core_processing
import core_indexing
from threading import Thread
import json
import re
# --- å…¨å±€é…ç½® ---
EMBEDDING_MODEL_NAME = "./models/bge-large-zh-v1.5"
LLM_MODEL_PATH = "./models/Qwen2.5-7B-Instruct"
STATIC_VECTOR_DB_PATH = "./vector_db"
TEMP_UPLOAD_DIR = "./temp_uploads"
SESSION_DB_BASE = "./vector_db"

# ã€æ–°å¢ã€‘åé¦ˆæ•°æ®åº“è·¯å¾„
FEEDBACK_DB_PATH = "./feedback_db"

# ã€æ–°å¢ã€‘Few-shot ç¤ºä¾‹
FEW_SHOT_EXAMPLES = [
    {
        "question": "ä»€ä¹ˆæ˜¯åå‘ä¼ æ’­ç®—æ³•ï¼Ÿ",
        "answer": """**åå‘ä¼ æ’­ç®—æ³•**æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œç”¨äºé«˜æ•ˆè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

**æ ¸å¿ƒæµç¨‹ï¼š**

1. **å‰å‘ä¼ æ’­**
   - è¾“å…¥æ•°æ®é€å±‚é€šè¿‡ç½‘ç»œ
   - æ¯å±‚è¿›è¡Œçº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°è®¡ç®—
   - æœ€ç»ˆå¾—åˆ°é¢„æµ‹è¾“å‡º

2. **è®¡ç®—æŸå¤±**
   - å¯¹æ¯”é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾
   - ä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€MSEï¼‰é‡åŒ–è¯¯å·®

3. **åå‘ä¼ æ’­**
   - ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå‘è¾“å…¥å±‚é€å±‚ä¼ é€’
   - åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   - âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚y Ã— âˆ‚y/âˆ‚w

4. **å‚æ•°æ›´æ–°**
   - ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨æ›´æ–°æƒé‡
   - w_new = w_old - learning_rate Ã— gradient

**å…³é”®ä¼˜åŠ¿ï¼š** é€šè¿‡ç¼“å­˜å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ã€‚"""
    },
    {
        "question": "Batch Normalizationå¦‚ä½•å·¥ä½œï¼Ÿ",
        "answer": """**Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰**æ˜¯ä¸€ç§å¼ºå¤§çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½æ˜¾è‘—æ”¹å–„æ·±åº¦ç½‘ç»œè®­ç»ƒã€‚

**å·¥ä½œæœºåˆ¶ï¼š**

1. **æ ‡å‡†åŒ–**
   - å¯¹æ¯ä¸ªbatchçš„æ¿€æ´»å€¼è¿›è¡Œæ ‡å‡†åŒ–
   - ä½¿å…¶å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
   - x_norm = (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)

2. **ç¼©æ”¾å’Œå¹³ç§»**
   - å¼•å…¥å¯å­¦ä¹ å‚æ•°Î³ï¼ˆscaleï¼‰å’ŒÎ²ï¼ˆshiftï¼‰
   - y = Î³ Ã— x_norm + Î²
   - å…è®¸ç½‘ç»œæ¢å¤åŸå§‹è¡¨ç¤ºèƒ½åŠ›

**ä¸»è¦ä¼˜åŠ¿ï¼š**

- **åŠ é€Ÿæ”¶æ•›**ï¼šç¨³å®šæ¿€æ´»åˆ†å¸ƒï¼Œå…è®¸ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡
- **å‡å°‘æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼šè§„èŒƒåŒ–æ¿€æ´»å€¼èŒƒå›´
- **æ­£åˆ™åŒ–æ•ˆåº”**ï¼šbatché—´çš„éšæœºæ€§äº§ç”Ÿç±»ä¼¼dropoutçš„æ•ˆæœ
- **é™ä½å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿåº¦**ï¼šä½¿ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒ

**åº”ç”¨åœºæ™¯ï¼š** é€šå¸¸æ”¾ç½®åœ¨çº¿æ€§å±‚ä¹‹åã€æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚"""
    }
]

GENERATION_CONFIG = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
}


# ==================== è¾…åŠ©å‡½æ•° ====================

def _display_question_result(result: Dict[str, Any], expanded: bool = False):
    """æ˜¾ç¤ºå•ä¸ªé¢˜ç›®çš„ç­”é¢˜ç»“æœ"""
    idx = result['question_index']
    question = result['question']
    options = result['options']
    user_ans_idx = result.get('user_answer', -1)
    correct_ans_idx = result['correct_answer']
    is_correct = result['is_correct']
    is_unanswered = result.get('is_unanswered', False)
    explanation = result['explanation']
    
    if is_correct:
        status_badge = "âœ… **æ­£ç¡®**"
    elif is_unanswered:
        status_badge = "â­• **æœªä½œç­”**"
    else:
        status_badge = "âŒ **é”™è¯¯**"
    
    with st.expander(f"ç¬¬ {idx+1} é¢˜ - {status_badge}", expanded=expanded):
        st.markdown(f"**é¢˜ç›®:** {question}")
        
        st.markdown("**é€‰é¡¹:**")
        for i, opt in enumerate(options):
            if is_unanswered:
                if i == correct_ans_idx:
                    st.markdown(f"- :green[**{opt}**] âœ… (æ­£ç¡®ç­”æ¡ˆ)")
                else:
                    st.markdown(f"- {opt}")
            elif i == user_ans_idx and i == correct_ans_idx:
                st.markdown(f"- :green[**{opt}**] âœ… (æ‚¨çš„ç­”æ¡ˆï¼Œæ­£ç¡®)")
            elif i == user_ans_idx:
                st.markdown(f"- :red[**{opt}**] âŒ (æ‚¨çš„ç­”æ¡ˆ)")
            elif i == correct_ans_idx:
                st.markdown(f"- :green[**{opt}**] âœ… (æ­£ç¡®ç­”æ¡ˆ)")
            else:
                st.markdown(f"- {opt}")
        
        st.markdown("**ğŸ“– è§£æ:**")
        st.info(explanation)


# ==================== æ¨¡å‹åŠ è½½ï¼ˆå…¨å±€ç¼“å­˜ï¼‰====================

@st.cache_resource
def load_llm(model_path: str):
    """åŠ è½½å¤§è¯­è¨€æ¨¡å‹"""
    print("ğŸ§  å¼€å§‹åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if device == 'cuda':
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
            model = model.to(device)
        
        model.eval()
        print("âœ“ å¤§è¯­è¨€æ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model, device
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


@st.cache_resource
def load_embedding_model(model_path: str):
    """åŠ è½½Embeddingæ¨¡å‹"""
    print("ğŸ“Š å¼€å§‹åŠ è½½Embeddingæ¨¡å‹...")
    embedding_model = core_indexing.initialize_embedding_model(model_path)
    print("âœ“ Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
    return embedding_model


# ==================== æ£€ç´¢å™¨åˆ›å»º ====================

class EnsembleRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼šå‘é‡æ£€ç´¢ + BM25"""
    def __init__(self, retrievers, weights=None):
        self.retrievers = retrievers
        self.weights = weights or [1.0] * len(retrievers)

    def invoke(self, query: str) -> List[Document]:
        all_docs = []
        for retriever, w in zip(self.retrievers, self.weights):
            try:
                docs = retriever.invoke(query)
            except Exception:
                docs = retriever.get_relevant_documents(query)
            all_docs.extend(docs * int(w * 10))

        unique_docs = {d.page_content: d for d in all_docs}
        return list(unique_docs.values())


def create_retriever_from_db(db: Chroma, embedding_model: HuggingFaceEmbeddings):
    """ä»Chromaæ•°æ®åº“åˆ›å»ºæ··åˆæ£€ç´¢å™¨"""
    
    vector_retriever = db.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 6}
    )
    
    try:
        all_data = db.get()
        if all_data and all_data.get('documents'):
            docs = [
                Document(page_content=doc, metadata=meta)
                for doc, meta in zip(
                    all_data['documents'],
                    all_data.get('metadatas', [{}] * len(all_data['documents']))
                )
            ]
            
            bm25_retriever = BM25Retriever.from_documents(docs)
            bm25_retriever.k = 6
            
            ensemble_retriever = EnsembleRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                weights=[0.6, 0.4]
            )
            print("âœ“ æ··åˆæ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ (å‘é‡ + BM25)")
            return ensemble_retriever
    except Exception as e:
        print(f"âš  BM25åˆ›å»ºå¤±è´¥: {e}ï¼Œä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
    
    return vector_retriever


def load_static_retriever(db_path: str, embedding_model: HuggingFaceEmbeddings):
    """åŠ è½½é™æ€çŸ¥è¯†åº“çš„æ£€ç´¢å™¨"""
    
    if not os.path.exists(db_path):
        return None
    
    db = Chroma(
        persist_directory=db_path,
        embedding_function=embedding_model
    )
    
    return create_retriever_from_db(db, embedding_model)


# ==================== RAGé—®ç­”åŠŸèƒ½ (ã€æ›¿æ¢ã€‘å¢å¼ºç‰ˆ) ====================

def generate_queries(original_query, num_queries=2):
    """æ™ºèƒ½æŸ¥è¯¢æ‰©å±•"""
    queries = [original_query]
    
    # è¡¥å……ç–‘é—®è¯
    if not original_query.startswith(("ä»€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "è¯·é—®", "èƒ½å¦", "æ€ä¹ˆ")):
        queries.append(f"ä»€ä¹ˆæ˜¯{original_query}")
    
    # æ·»åŠ è§£é‡Šæ€§æŸ¥è¯¢
    if "è§£é‡Š" not in original_query and "ä»‹ç»" not in original_query:
        queries.append(f"è¯·è§£é‡Š{original_query}")
    
    # æ·»åŠ é¢†åŸŸå‰ç¼€
    domain_keywords = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "ç®—æ³•"]
    has_domain = any(kw in original_query for kw in domain_keywords)
    
    if not has_domain and len(queries) < num_queries + 1:
        queries.append(f"æ·±åº¦å­¦ä¹ ä¸­çš„{original_query}")
    
    return queries[:num_queries + 1]


def smart_context_selection(docs, query, max_docs=4):
    """æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©ï¼šå¤šç»´åº¦è¯„åˆ†"""
    if len(docs) <= max_docs:
        return docs
    
    query_terms = set(query.lower().split())
    
    scored_docs = []
    for doc in docs:
        content_lower = doc.page_content.lower()
        
        # 1. å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_score = sum(1 for term in query_terms if term in content_lower)
        
        # 2. æ–‡æ¡£é•¿åº¦å¾—åˆ†ï¼ˆæ›´å®Œæ•´çš„ä¿¡æ¯ï¼‰
        length_score = min(len(doc.page_content) / 1000, 2.0)
        
        # 3. æ–‡æ¡£å¤šæ ·æ€§ï¼ˆé¿å…é‡å¤ï¼‰
        diversity_score = 1.0
        
        total_score = keyword_score * 2 + length_score + diversity_score
        scored_docs.append((total_score, doc))
    
    scored_docs.sort(reverse=True, key=lambda x: x[0])
    return [doc for _, doc in scored_docs[:max_docs]]


def extract_dialogue_context(messages, max_history=3):
    """æå–å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡"""
    if len(messages) < 3:
        return None
    
    recent_messages = messages[-(2*max_history):]
    
    context_parts = []
    for i in range(0, len(recent_messages), 2):
        if i+1 < len(recent_messages):
            user_msg = recent_messages[i]["content"][:150]
            assistant_msg = recent_messages[i+1]["content"][:150]
            context_parts.append(f"Q: {user_msg}\nA: {assistant_msg}")
    
    return "\n\n".join(context_parts) if context_parts else None


def retrieve_with_enhancements(retriever, query: str, k: int = 4, enable_expansion: bool = True):
    """å¢å¼ºæ£€ç´¢ (æ¥è‡ª module_rag_assistant.py)"""
    try:
        all_docs = []
        seen_content = set()
        
        if enable_expansion:
            queries = generate_queries(query, num_queries=2)
        else:
            queries = [query]
        
        for q in queries:
            docs = retriever.invoke(q)
            
            for doc in docs:
                content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
                if content_hash not in seen_content:
                    all_docs.append(doc)
                    seen_content.add(content_hash)
        
        final_docs = smart_context_selection(all_docs, query, max_docs=k)
        
        context_parts = []
        sources = []
        
        for i, doc in enumerate(final_docs, 1):
            source = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', 'N/A')
            
            context_parts.append(f"[æ–‡æ¡£ {i}]\n{doc.page_content}")
            sources.append(f"{source} (é¡µç : {page})")
        
        context = "\n\n".join(context_parts)
        
        return context, sources, final_docs
        
    except Exception as e:
        st.error(f"æ£€ç´¢å‡ºé”™: {e}")
        return "", [], []


def build_enhanced_prompt(context, question, dialogue_history=None, 
                         use_fewshot=True, use_multi_turn=True):
    """æ„å»ºä¼˜åŒ–çš„prompt"""
    
    system_prompt = """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ä¸“å®¶æ•™å¸ˆã€‚ä½ çš„ä½¿å‘½æ˜¯å¸®åŠ©å­¦ä¹ è€…æ·±å…¥ç†è§£å¤æ‚çš„æŠ€æœ¯æ¦‚å¿µã€‚

**æ•™å­¦åŸåˆ™ï¼š**

1. **å‡†ç¡®æ€§æ˜¯åŸºç¡€**
   - ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”
   - ä¸ç¼–é€ æˆ–è‡†æµ‹è¶…å‡ºèµ„æ–™èŒƒå›´çš„å†…å®¹
   - é‡åˆ°èµ„æ–™ä¸è¶³æ—¶ï¼Œè¯šå®è¯´æ˜å¹¶å»ºè®®æŸ¥é˜…æ–¹å‘

2. **ç»“æ„åŒ–è¡¨è¾¾**
   - ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œå±‚æ¬¡ç»„ç»‡å†…å®¹
   - å…ˆæ¦‚è¿°æ ¸å¿ƒæ¦‚å¿µï¼Œå†å±•å¼€ç»†èŠ‚
   - å–„ç”¨**åŠ ç²—**ã€ç¼–å·åˆ—è¡¨ã€åˆ†ç‚¹è¯´æ˜

3. **æ·±å…¥æµ…å‡º**
   - å¤æ‚æ¦‚å¿µå…ˆç»™å‡ºç›´è§‚è§£é‡Š
   - é€‚æ—¶ä½¿ç”¨ç±»æ¯”å’Œå®ä¾‹å¸®åŠ©ç†è§£
   - å¿…è¦æ—¶æŒ‡å‡ºæ•°å­¦åŸç†ï¼Œä½†ä¿æŒå¯è¯»æ€§

4. **ç†è®ºè”ç³»å®è·µ**
   - è¯´æ˜æ¦‚å¿µçš„å®é™…åº”ç”¨åœºæ™¯
   - æŒ‡å‡ºå¸¸è§è¯¯åŒºå’Œæ³¨æ„äº‹é¡¹
   - æä¾›è¿›ä¸€æ­¥å­¦ä¹ çš„æ–¹å‘

5. **å¯¹è¯è¿è´¯æ€§**ï¼ˆå¤šè½®å¯¹è¯æ—¶ï¼‰
   - å‚è€ƒä¹‹å‰è®¨è®ºçš„å†…å®¹
   - é€æ­¥æ·±å…¥ï¼Œé¿å…é‡å¤
   - å›ç­”æ—¶å‘¼åº”å­¦ä¹ è€…çš„é—®é¢˜è„‰ç»œ

**å›ç­”é£æ ¼ï¼š** ä¸“ä¸šè€Œå‹å¥½ï¼Œåƒä¸€ä½è€å¿ƒçš„å¯¼å¸ˆä¸å­¦ç”Ÿé¢å¯¹é¢äº¤æµã€‚"""

    # Few-shotç¤ºä¾‹
    fewshot_text = ""
    if use_fewshot:
        fewshot_text = "\n\n**å‚è€ƒç¤ºä¾‹ï¼š**\n"
        for i, example in enumerate(FEW_SHOT_EXAMPLES[:2], 1):
            fewshot_text += f"\nã€ç¤ºä¾‹ {i}ã€‘\né—®ï¼š{example['question']}\nç­”ï¼š{example['answer'][:300]}...\n"
    
    # å¯¹è¯å†å²
    history_section = ""
    if use_multi_turn and dialogue_history:
        history_section = f"\n\n**ä¹‹å‰çš„å¯¹è¯ï¼š**\n{dialogue_history}\n"
    
    user_message = f"""{fewshot_text}

**å‚è€ƒèµ„æ–™ï¼š**
{context}{history_section}

---

**å½“å‰é—®é¢˜ï¼š** {question}

è¯·åŸºäºå‚è€ƒèµ„æ–™ï¼Œæä¾›ä¸€ä¸ªä¸“ä¸šã€å‡†ç¡®ä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    return messages


def generate_response_stream(tokenizer, model, device, messages):
    """æµå¼ç”Ÿæˆå“åº”"""
    try:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        
        generation_kwargs = {
            **inputs,
            **GENERATION_CONFIG,
            "streamer": streamer,
        }
        
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        for text_chunk in streamer:
            yield text_chunk
            
    except Exception as e:
        yield f"ç”Ÿæˆå‡ºé”™: {e}"


def save_feedback(question, answer, feedback_type, comment=""):
    """ä¿å­˜ç”¨æˆ·åé¦ˆ"""
    try:
        os.makedirs(FEEDBACK_DB_PATH, exist_ok=True)
        
        feedback_data = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer[:200], # åªä¿å­˜ç®€ç•¥ç­”æ¡ˆ
            "type": feedback_type,
            "comment": comment
        }
        
        feedback_file = os.path.join(
            FEEDBACK_DB_PATH,
            f"feedback_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.json"
        )
        
        with open(feedback_file, 'w', encoding='utf-8') as f:
            json.dump(feedback_data, f, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        st.error(f"ä¿å­˜åé¦ˆå¤±è´¥: {e}")
        return False


# ==================== ä¾§è¾¹æ é…ç½® (ã€æ›¿æ¢ã€‘å¢å¼ºç‰ˆ) ====================

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ é…ç½®"""
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
        
        st.divider()
        
        # --- RAG é—®ç­”è®¾ç½® ---
        st.subheader("ğŸ¤– AIåŠ©æ•™è®¾ç½®")
        
        enable_query_expansion = st.checkbox(
            "å¯ç”¨æŸ¥è¯¢æ‰©å±•",
            value=True,
            help="è‡ªåŠ¨ç”Ÿæˆç›¸å…³æŸ¥è¯¢ï¼Œæé«˜æ£€ç´¢è¦†ç›–ç‡"
        )
        
        enable_multi_turn = st.checkbox(
            "å¤šè½®å¯¹è¯ä¼˜åŒ–",
            value=True,
            help="åœ¨å¯¹è¯ä¸­è€ƒè™‘å†å²ä¸Šä¸‹æ–‡"
        )
        
        if enable_multi_turn:
            max_history_turns = st.slider(
                "å¯¹è¯å†å²è½®æ•°",
                min_value=1,
                max_value=5,
                value=3
            )
        else:
            max_history_turns = 0
        
        use_fewshot = st.checkbox(
            "Few-shotç¤ºä¾‹",
            value=True,
            help="åœ¨promptä¸­åŒ…å«ç¤ºä¾‹å›ç­”"
        )
        
        k_documents = st.slider(
            "æ£€ç´¢æ–‡æ¡£æ•°é‡",
            min_value=2,
            max_value=8,
            value=4,
            help="æ¯æ¬¡æ£€ç´¢è¿”å›çš„æ–‡æ¡£æ•°é‡"
        )
        
        st.divider()
        
        # --- ç”Ÿæˆå‚æ•° ---
        st.subheader("ğŸšï¸ ç”Ÿæˆå‚æ•°")
        
        temperature = st.slider(
            "Temperature",
            min_value=0.1,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="æ§åˆ¶å›ç­”çš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šå¤šæ ·åŒ–"
        )
        
        GENERATION_CONFIG['temperature'] = temperature
        
        st.divider()
        
        # --- çŸ¥è¯†åº“çŠ¶æ€ (æ¥è‡ªåŸ main_app.py) ---
        st.subheader("ğŸ“š çŸ¥è¯†åº“çŠ¶æ€")
        if st.session_state.get('quiz_retriever'):
            st.success("âœ… å‡ºé¢˜åº“å·²åŠ è½½")
        else:
            st.info("â³ å‡ºé¢˜åº“æœªåŠ è½½")
            
        if st.session_state.get('rag_retriever'):
            st.success("âœ… é—®ç­”åº“å·²åŠ è½½")
        else:
            st.info("â³ é—®ç­”åº“æœªåŠ è½½")
        
        st.divider()
        
        # --- ä¼šè¯æ§åˆ¶ ---
        st.subheader("ğŸ”„ ä¼šè¯æ§åˆ¶")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                st.session_state.rag_messages = []
                st.rerun()

        with col2:
            if st.button("ğŸ”„ é‡ç½®ä¼šè¯", use_container_width=True):
                if st.session_state.session_db_path and "session_" in st.session_state.session_db_path:
                    if os.path.exists(st.session_state.session_db_path):
                        shutil.rmtree(st.session_state.session_db_path)
                
                for key in list(st.session_state.keys()):
                    if key != 'models_loaded':
                        del st.session_state[key]
                
                st.rerun()
        
        # ã€ä¿®æ”¹ã€‘è¿”å›æ‰€æœ‰RAGé…ç½®
        return (enable_query_expansion, k_documents, 
                enable_multi_turn, max_history_turns, use_fewshot)


# ==================== ä¸»åº”ç”¨ ====================

def main():
    st.set_page_config(
        page_title="ä¸ªæ€§åŒ–å­¦ä¹ æµ‹éªŒç³»ç»Ÿ",
        page_icon="ğŸ“˜",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ“˜ ä¸ªæ€§åŒ–å­¦ä¹ æµ‹éªŒç³»ç»Ÿ")
    st.caption("ä¸Šä¼ æ•™æ â†’ æ™ºèƒ½å‡ºé¢˜ â†’ è‡ªåŠ¨è¯„åˆ† â†’ AIç­”ç–‘")
    
    # --- åˆå§‹åŒ–å…¨å±€æ¨¡å‹ ---
    if 'models_loaded' not in st.session_state:
        with st.status("ç³»ç»Ÿåˆå§‹åŒ–ä¸­...", expanded=True) as status:
            st.write("ğŸ§  åŠ è½½è¯­è¨€æ¨¡å‹...")
            tokenizer, model, device = load_llm(LLM_MODEL_PATH)
            st.session_state.llm_tokenizer = tokenizer
            st.session_state.llm_model = model
            st.session_state.device = device
            
            st.write("ğŸ“Š åŠ è½½æ£€ç´¢æ¨¡å‹...")
            st.session_state.embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
            
            st.session_state.models_loaded = True
            status.update(label="âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ", state="complete", expanded=False)
    
    # --- åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ ---
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    
    if 'session_db_path' not in st.session_state:
        st.session_state.session_db_path = None
    
    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨åŒæ£€ç´¢å™¨æ¶æ„
    if 'quiz_retriever' not in st.session_state:
        st.session_state.quiz_retriever = None  # ä»…PDFï¼Œç”¨äºå‡ºé¢˜
        
    if 'rag_retriever' not in st.session_state:
        st.session_state.rag_retriever = None  # æ··åˆçŸ¥è¯†åº“ï¼Œç”¨äºé—®ç­”
    
    # ã€æ–°å¢ã€‘ç¼“å­˜é»˜è®¤æ•™ææ£€ç´¢å™¨
    if 'static_retriever' not in st.session_state:
        st.session_state.static_retriever = None
    
    if 'quiz_questions' not in st.session_state:
        st.session_state.quiz_questions = []
    
    if 'quiz_report' not in st.session_state:
        st.session_state.quiz_report = None
    
    if 'rag_messages' not in st.session_state:
        st.session_state.rag_messages = []
        
    # ã€æ–°å¢ã€‘ä¸ºæ¨è 2 åšå‡†å¤‡
    if 'queued_rag_question' not in st.session_state:
        st.session_state.queued_rag_question = None
    
    # --- ä¾§è¾¹æ é…ç½® (ã€ä¿®æ”¹ã€‘æ¥æ”¶5ä¸ªè¿”å›å€¼) ---
    (enable_query_expansion, k_documents, 
     enable_multi_turn, max_history_turns, use_fewshot) = render_sidebar()
    
    # --- åˆ›å»ºæ ‡ç­¾é¡µ ---
    tab_upload, tab_quiz, tab_report, tab_rag = st.tabs([
        "ğŸ“š ä¸Šä¼ æ•™æ",
        "ğŸ“ å¼€å§‹æµ‹éªŒ",
        "ğŸ“Š å­¦ä¹ æŠ¥å‘Š",
        "ğŸ¤– AIåŠ©æ•™"
    ])
    
    # ==================== æ ‡ç­¾é¡µ1ï¼šä¸Šä¼ æ•™æ ====================
    with tab_upload:
        st.header("ğŸ“š ä¸Šä¼ å­¦ä¹ æ•™æ")
        st.info("ğŸ’¡ **åŒçŸ¥è¯†åº“æ¶æ„**ï¼šä¸Šä¼ PDFåå°†åˆ›å»ºä¸¤ä¸ªçŸ¥è¯†åº“ - ä¸€ä¸ªä¸“é—¨ç”¨äºå‡ºé¢˜ï¼ˆä»…PDFï¼‰ï¼Œå¦ä¸€ä¸ªç”¨äºAIé—®ç­”ï¼ˆé»˜è®¤æ•™æ+PDFæ··åˆï¼‰")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            uploaded_file = st.file_uploader(
                "é€‰æ‹©PDFæ–‡ä»¶",
                type="pdf",
                help="æ”¯æŒæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç›¸å…³æ•™æ"
            )
            
            if uploaded_file is not None:
                st.success(f"âœ“ å·²é€‰æ‹©: {uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")
                
                # æ˜¾ç¤ºé¢„ä¼°å¤„ç†æ—¶é—´
                estimated_time = max(1, int(uploaded_file.size / (1024 * 1024)))  # ç²—ç•¥ä¼°è®¡ï¼š1MB/åˆ†é’Ÿ
                st.caption(f"â±ï¸ é¢„ä¼°å¤„ç†æ—¶é—´: {estimated_time}-{estimated_time*2} åˆ†é’Ÿ")
                
                if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
                    # ä¿å­˜æ–‡ä»¶
                    os.makedirs(TEMP_UPLOAD_DIR, exist_ok=True)
                    temp_path = os.path.join(TEMP_UPLOAD_DIR, f"{st.session_state.session_id}.pdf")
                    
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getvalue())
                    
                    # æäº¤åå°ä»»åŠ¡
                    from background_processor import submit_pdf_task
                    
                    task_id = submit_pdf_task(temp_path, uploaded_file.name, st.session_state.session_id)
                    st.session_state.processing_task_id = task_id
                    
                    st.success("âœ… ä¸Šä¼ æˆåŠŸï¼æ‚¨çš„æ•™ææ­£åœ¨åå°å¤„ç†ä¸­...")
                    st.info(f"""
ğŸ’¡ **å¤„ç†è¯´æ˜ï¼š**
- æ•™ææ­£åœ¨åå°å¤„ç†ï¼Œè¿™å¤§çº¦éœ€è¦ {estimated_time}-{estimated_time*2} åˆ†é’Ÿ
- æ‚¨å¯ä»¥å…³é—­æ­¤é¡µé¢ï¼Œç¨åå†å›æ¥æŸ¥çœ‹
- å¤„ç†å®Œæˆåå°†è‡ªåŠ¨é€šçŸ¥æ‚¨
- ä»»åŠ¡ID: `{task_id[:16]}...`
                    """)
                    st.rerun()
            
            # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
            if 'processing_task_id' in st.session_state:
                st.divider()
                st.markdown("### ğŸ“Š å¤„ç†çŠ¶æ€")
                
                from background_processor import get_task_status, TaskStatus
                
                task = get_task_status(st.session_state.processing_task_id)
                
                if task:
                    # çŠ¶æ€æ˜¾ç¤º
                    if task.status == TaskStatus.PROCESSING:
                        st.info(f"â³ å¤„ç†ä¸­... {task.progress}%")
                        st.progress(task.progress / 100)
                        st.caption(f"å½“å‰: {task.message}")
                        
                        # è‡ªåŠ¨åˆ·æ–°
                        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€"):
                            st.rerun()
                        
                        st.caption("ğŸ’¡ é¡µé¢å°†è‡ªåŠ¨åˆ·æ–°ï¼Œè¯·ç¨å€™...")
                        
                    elif task.status == TaskStatus.COMPLETED:
                        st.success("âœ… å¤„ç†å®Œæˆï¼")
                        st.balloons()
                        
                        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åŠ è½½åŒçŸ¥è¯†åº“
                        if task.db_path:
                            embedding_model = st.session_state.embedding_model
                            
                            # 1. ã€æ–°å¢ã€‘ç¡®ä¿é»˜è®¤æ•™ææ£€ç´¢å™¨å·²åŠ è½½
                            if st.session_state.static_retriever is None:
                                print("ğŸ“š é¦–æ¬¡åŠ è½½é»˜è®¤æ•™ææ£€ç´¢å™¨...")
                                static_retriever = load_static_retriever(STATIC_VECTOR_DB_PATH, embedding_model)
                                st.session_state.static_retriever = static_retriever
                            else:
                                static_retriever = st.session_state.static_retriever
                            
                            # 2. åˆ›å»º"ä»…PDF"çš„æ£€ç´¢å™¨ï¼ˆç”¨äºå‡ºé¢˜ï¼‰
                            session_db = Chroma(
                                persist_directory=task.db_path,
                                embedding_function=embedding_model
                            )
                            session_retriever = create_retriever_from_db(session_db, embedding_model)
                            
                            # 3. ã€ä¿®æ”¹ã€‘è®¾ç½®å‡ºé¢˜æ£€ç´¢å™¨ï¼ˆä»…PDFï¼‰
                            st.session_state.quiz_retriever = session_retriever
                            print("âœ“ å‡ºé¢˜æ£€ç´¢å™¨å·²è®¾ç½® (ä»…PDF)")
                            
                            # 4. ã€æ–°å¢ã€‘åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼ˆé»˜è®¤ + PDFï¼Œç”¨äºRAGé—®ç­”ï¼‰
                            if static_retriever:
                                hybrid_rag_retriever = EnsembleRetriever(
                                    retrievers=[static_retriever, session_retriever],
                                    weights=[0.5, 0.5]  # å¯è°ƒæ•´æƒé‡
                                )
                                st.session_state.rag_retriever = hybrid_rag_retriever
                                print("âœ“ æ··åˆRAGæ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ (é»˜è®¤æ•™æ + PDF)")
                            else:
                                # å¦‚æœé»˜è®¤åº“åŠ è½½å¤±è´¥ï¼ŒRAGä¹Ÿåªèƒ½ç”¨PDF
                                st.session_state.rag_retriever = session_retriever
                                print("âš  é»˜è®¤åº“æœªåŠ è½½ï¼ŒRAGä½¿ç”¨PDFåº“")
                            
                            st.session_state.session_db_path = task.db_path
                            st.session_state.rag_messages = []
                            
                            st.success(f"ğŸ‰ åŒçŸ¥è¯†åº“å·²å°±ç»ªï¼å…±ç”Ÿæˆ {task.chunk_count} ä¸ªçŸ¥è¯†å—")
                            st.info("""
ğŸ“š **çŸ¥è¯†åº“è¯´æ˜ï¼š**
- ğŸ¯ å‡ºé¢˜çŸ¥è¯†åº“ï¼šä»…ä½¿ç”¨æ‚¨ä¸Šä¼ çš„PDF
- ğŸ¤– é—®ç­”çŸ¥è¯†åº“ï¼šæ··åˆé»˜è®¤æ•™æ + æ‚¨çš„PDFï¼ˆæ›´å…¨é¢ï¼‰
                            """)
                            st.info("ğŸ‘‰ è¯·åˆ‡æ¢åˆ° **ã€Œå¼€å§‹æµ‹éªŒã€** æˆ– **ã€ŒAIåŠ©æ•™ã€** æ ‡ç­¾é¡µ")
                            
                            # æ¸…ç†ä»»åŠ¡çŠ¶æ€
                            del st.session_state.processing_task_id
                        
                    elif task.status == TaskStatus.FAILED:
                        st.error(f"âŒ å¤„ç†å¤±è´¥: {task.error}")
                        
                        if st.button("ğŸ”„ é‡æ–°å°è¯•"):
                            del st.session_state.processing_task_id
                            st.rerun()
                    
                    elif task.status == TaskStatus.PENDING:
                        st.info("â° ç­‰å¾…å¤„ç†...")
                        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€"):
                            st.rerun()
        
        with col2:
            st.markdown("### ä½¿ç”¨é»˜è®¤æ•™æ")
            st.caption("åŒ…å«ç»å…¸ML/DLæ•™æ")
            
            if st.button("ğŸ“– åŠ è½½é»˜è®¤æ•™æ", use_container_width=True):
                with st.spinner("æ­£åœ¨åŠ è½½..."):
                    embedding_model = st.session_state.embedding_model
                    
                    retriever = load_static_retriever(STATIC_VECTOR_DB_PATH, embedding_model)
                    
                    if retriever:
                        st.session_state.session_db_path = STATIC_VECTOR_DB_PATH
                        
                        # ã€ä¿®æ”¹ã€‘ç¼“å­˜é»˜è®¤æ•™ææ£€ç´¢å™¨
                        st.session_state.static_retriever = retriever
                        
                        # ã€ä¿®æ”¹ã€‘ä¸¤ä¸ªæ£€ç´¢å™¨éƒ½æŒ‡å‘é»˜è®¤æ•™æ
                        st.session_state.quiz_retriever = retriever
                        st.session_state.rag_retriever = retriever
                        
                        st.session_state.rag_messages = []
                        
                        st.success("âœ… é»˜è®¤æ•™æåŠ è½½æˆåŠŸ")
                        st.info("ğŸ‘‰ å¯ä»¥å¼€å§‹ä½¿ç”¨æµ‹éªŒæˆ–é—®ç­”åŠŸèƒ½")
                    else:
                        st.error("âŒ åŠ è½½å¤±è´¥")
    
    # ==================== æ ‡ç­¾é¡µ2ï¼šå¼€å§‹æµ‹éªŒ ====================
    with tab_quiz:
        st.header("ğŸ“ ä¸ªæ€§åŒ–æµ‹éªŒ")
        
        # ã€ä¿®æ”¹ã€‘æ£€æŸ¥å‡ºé¢˜çŸ¥è¯†åº“
        if st.session_state.quiz_retriever is None:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æˆ–åŠ è½½æ•™æ")
        else:
            # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„çŸ¥è¯†åº“ç±»å‹
            if st.session_state.session_db_path == STATIC_VECTOR_DB_PATH:
                st.success("âœ“ å‡ºé¢˜çŸ¥è¯†åº“ï¼šé»˜è®¤æ•™æ")
            else:
                st.success("âœ“ å‡ºé¢˜çŸ¥è¯†åº“ï¼šæ‚¨ä¸Šä¼ çš„PDF")
            
            if 'quiz_stage' not in st.session_state:
                st.session_state.quiz_stage = 'config'
            
            # ==================== é…ç½®æµ‹éªŒ ====================
            if st.session_state.quiz_stage == 'config':
                st.subheader("ğŸ¯ æµ‹éªŒé…ç½®")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### é¢˜ç›®è®¾ç½®")
                    num_choice = st.slider("é€‰æ‹©é¢˜æ•°é‡", 1, 10, 3)
                    num_boolean = st.slider("åˆ¤æ–­é¢˜æ•°é‡", 1, 10, 2)
                    total_questions = num_choice + num_boolean
                    st.info(f"ğŸ“ æ€»é¢˜æ•°: **{total_questions}** é“")
                
                with col2:
                    st.markdown("#### éš¾åº¦è®¾ç½®")
                    difficulty = st.select_slider(
                        "é€‰æ‹©éš¾åº¦",
                        options=["easy", "medium", "hard"],
                        value="medium",
                        format_func=lambda x: {"easy": "ğŸŸ¢ ç®€å•", "medium": "ğŸŸ¡ ä¸­ç­‰", "hard": "ğŸ”´ å›°éš¾"}[x]
                    )
                    
                    st.markdown("""
                    - ğŸŸ¢ **ç®€å•**: åŸºç¡€æ¦‚å¿µ
                    - ğŸŸ¡ **ä¸­ç­‰**: æ¦‚å¿µåº”ç”¨
                    - ğŸ”´ **å›°éš¾**: æ·±åº¦åˆ†æ
                    """)
                
                st.divider()
                
                col_btn1, col_btn2, col_btn3 = st.columns([1, 2, 1])
                
                with col_btn2:
                    if st.button("ğŸš€ ç”Ÿæˆæµ‹éªŒ", type="primary", use_container_width=True):
                        with st.spinner("ğŸ¯ æ­£åœ¨ç”Ÿæˆé¢˜ç›®..."):
                            try:
                                # ã€ä¿®æ”¹ã€‘ä½¿ç”¨å‡ºé¢˜çŸ¥è¯†åº“
                                questions = question_generator.generate_quiz_questions(
                                    retriever=st.session_state.quiz_retriever,
                                    tokenizer=st.session_state.llm_tokenizer,
                                    model=st.session_state.llm_model,
                                    device=st.session_state.device,
                                    num_choice=num_choice,
                                    num_boolean=num_boolean,
                                    difficulty=difficulty,
                                    max_retries=3
                                )
                                
                                if questions and len(questions) > 0:
                                    st.session_state.quiz_questions = questions
                                    st.session_state.quiz_stage = 'answering'
                                    st.session_state.quiz_report = None
                                    st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(questions)} é“é¢˜ç›®")
                                    st.rerun()
                                else:
                                    st.error("âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•")
                            
                            except Exception as e:
                                st.error(f"âŒ å‡ºé”™: {e}")
            
            # ==================== ç­”é¢˜ä¸­ ====================
            elif st.session_state.quiz_stage == 'answering':
                questions = st.session_state.quiz_questions
                
                col_info1, col_info2, col_info3 = st.columns([2, 2, 1])
                
                with col_info1:
                    st.metric("ğŸ“ é¢˜ç›®æ€»æ•°", f"{len(questions)} é“")
                
                with col_info2:
                    choice_count = sum(1 for q in questions if q['type'] == 'choice')
                    boolean_count = len(questions) - choice_count
                    st.metric("ğŸ“‹ é¢˜å‹", f"é€‰æ‹© {choice_count} / åˆ¤æ–­ {boolean_count}")
                
                with col_info3:
                    if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆ"):
                        st.session_state.quiz_stage = 'config'
                        st.session_state.quiz_questions = []
                        st.rerun()
                
                st.divider()
                
                with st.form("quiz_form"):
                    st.markdown("### ğŸ“ è¯·ä½œç­”")
                    
                    user_answers_list = []
                    
                    for i, question in enumerate(questions):
                        q_type_emoji = "ğŸ“‹" if question["type"] == "choice" else "â“"
                        q_type_text = "é€‰æ‹©é¢˜" if question["type"] == "choice" else "åˆ¤æ–­é¢˜"
                        
                        st.markdown(f"#### {q_type_emoji} ç¬¬ {i+1} é¢˜ ({q_type_text})")
                        st.markdown(f"**{question['question']}**")
                        
                        options = question["options"]
                        
                        selected = st.radio(
                            f"è¯·é€‰æ‹©ç­”æ¡ˆï¼ˆç¬¬{i+1}é¢˜ï¼‰",
                            options=options,
                            key=f"q_{i}",
                            index=None,
                            label_visibility="collapsed"
                        )
                        
                        user_answers_list.append(selected)
                        st.divider()
                    
                    col_submit1, col_submit2, col_submit3 = st.columns([1, 2, 1])
                    
                    with col_submit2:
                        submitted = st.form_submit_button(
                            "ğŸ“Š æäº¤æµ‹éªŒ",
                            type="primary",
                            use_container_width=True
                        )
                    
                    if submitted:
                        unanswered_count = user_answers_list.count(None)
                        
                        if unanswered_count > 0:
                            st.warning(f"âš ï¸ è¿˜æœ‰ {unanswered_count} é“é¢˜æœªä½œç­”")
                        
                        try:
                            score_data = evaluator.grade_quiz(questions, user_answers_list)
                            st.session_state.quiz_report = score_data
                            st.session_state.quiz_stage = 'completed'
                            st.balloons()
                            st.rerun()
                        except Exception as e:
                            st.error(f"âŒ åˆ¤åˆ†å¤±è´¥: {e}")
            
            # ==================== å·²å®Œæˆ ====================
            elif st.session_state.quiz_stage == 'completed':
                if st.session_state.quiz_report is None:
                    st.error("âŒ æ‰¾ä¸åˆ°æµ‹éªŒç»“æœ")
                    st.session_state.quiz_stage = 'config'
                    st.rerun()
                
                report = st.session_state.quiz_report
                
                st.subheader("ğŸ‰ æµ‹éªŒå®Œæˆ")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ğŸ“Š æ€»åˆ†", f"{report['score_percentage']:.1f}%")
                
                with col2:
                    st.metric("âœ… æ­£ç¡®", f"{report['correct']} é¢˜")
                
                with col3:
                    st.metric("âŒ é”™è¯¯", f"{report['wrong']} é¢˜")
                
                with col4:
                    if report.get('unanswered', 0) > 0:
                        st.metric("â­• æœªç­”", f"{report['unanswered']} é¢˜")
                    else:
                        st.metric("ğŸ“ æ€»æ•°", f"{report['total']} é¢˜")
                
                from quiz_module.evaluator import get_performance_level
                performance = get_performance_level(report['score_percentage'])
                
                st.markdown(f"### è¯„çº§: :{performance['color']}[{performance['emoji']} {performance['level']}]")
                st.info(performance['message'])
                
                st.divider()
                
                st.subheader("ğŸ“‹ ç­”é¢˜è¯¦æƒ…")
                
                correct_results = [r for r in report['results'] if r['is_correct']]
                wrong_results = [r for r in report['results'] if not r['is_correct'] and not r.get('is_unanswered', False)]
                
                tab_wrong, tab_correct, tab_all = st.tabs([
                    f"âŒ é”™é¢˜ ({len(wrong_results)})",
                    f"âœ… æ­£ç¡® ({len(correct_results)})",
                    f"ğŸ“ å…¨éƒ¨ ({report['total']})"
                ])
                
                with tab_wrong:
                    if len(wrong_results) == 0:
                        st.success("ğŸ‰ æ²¡æœ‰é”™é¢˜ï¼")
                    else:
                        for result in wrong_results:
                            _display_question_result(result, expanded=True)
                
                with tab_correct:
                    if len(correct_results) == 0:
                        st.warning("ğŸ˜… åŠ æ²¹ï¼")
                    else:
                        for result in correct_results:
                            _display_question_result(result, expanded=False)
                
                with tab_all:
                    for result in report['results']:
                        _display_question_result(result, expanded=not result['is_correct'])
                
                st.divider()
                
                col_btn1, col_btn2, col_btn3 = st.columns(3)
                
                with col_btn1:
                    if st.button("ğŸ“Š æŸ¥çœ‹æŠ¥å‘Š", use_container_width=True):
                        st.info("ğŸ‘‰ è¯·åˆ‡æ¢åˆ°ã€Œå­¦ä¹ æŠ¥å‘Šã€æ ‡ç­¾é¡µ")
                
                with col_btn2:
                    if st.button("ğŸ”„ é‡æ–°æµ‹éªŒ", use_container_width=True):
                        st.session_state.quiz_stage = 'config'
                        st.session_state.quiz_questions = []
                        st.session_state.quiz_report = None
                        st.rerun()
                
                with col_btn3:
                    if st.button("ğŸ’¾ å¯¼å‡ºç»“æœ", use_container_width=True):
                        import json
                        
                        export_data = {
                            "timestamp": datetime.now().isoformat(),
                            "score": report,
                            "questions": st.session_state.quiz_questions
                        }
                        
                        json_str = json.dumps(export_data, ensure_ascii=False, indent=2)
                        
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ (JSON)",
                            data=json_str,
                            file_name=f"quiz_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json",
                            use_container_width=True
                        )

    # ==================== æ ‡ç­¾é¡µ3ï¼šå­¦ä¹ æŠ¥å‘Š ====================
    with tab_report:
        st.header("ğŸ“Š å­¦ä¹ æŠ¥å‘Š")
        
        if st.session_state.quiz_report is None:
            st.info("ğŸ“ å®Œæˆæµ‹éªŒåå°†æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š")
            
            st.markdown("""
            **æŠ¥å‘Šå†…å®¹ï¼š**
            - ğŸ“ˆ æˆç»©å’Œè¯„çº§
            - ğŸ¯ çŸ¥è¯†ç‚¹æŒæ¡åº¦åˆ†æ
            - ğŸ’¡ è–„å¼±çŸ¥è¯†ç‚¹è¯†åˆ«
            - ğŸ“š ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®
            - ğŸ“Š å¯è§†åŒ–å›¾è¡¨
            """)
        else:
            report = st.session_state.quiz_report
            
            st.success("âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ")
            
            st.subheader("ğŸ“ˆ æµ‹éªŒæ¦‚è§ˆ")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("ğŸ“ æ€»é¢˜æ•°", f"{report['total']}")
            
            with col2:
                st.metric("âœ… æ­£ç¡®", f"{report['correct']}", delta=f"+{report['correct']}")
            
            with col3:
                st.metric("âŒ é”™è¯¯", f"{report['wrong']}", delta=f"-{report['wrong']}" if report['wrong'] > 0 else "0")
            
            with col4:
                st.metric("ğŸ’¯ å¾—åˆ†", f"{report['score_percentage']:.1f}%")
            
            with col5:
                from quiz_module.evaluator import get_performance_level
                performance = get_performance_level(report['score_percentage'])
                st.metric("ğŸ† è¯„çº§", f"{performance['emoji']} {performance['level']}")
            
            st.divider()
            
            st.subheader("ğŸ“Š æ•°æ®å¯è§†åŒ–")
            
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                st.markdown("#### ğŸ“‹ ç­”é¢˜åˆ†å¸ƒ")
                
                chart_df = report_generator.prepare_chart_data(report)
                
                if not chart_df.empty:
                    chart = alt.Chart(chart_df).mark_bar().encode(
                        x=alt.X('ç±»åˆ«', axis=alt.Axis(labelAngle=0)),
                        y=alt.Y('æ•°é‡'),
                        tooltip=['ç±»åˆ«', 'æ•°é‡']
                    ).interactive()
                    st.altair_chart(chart, use_container_width=True)
                else:
                    st.info("æš‚æ— æ•°æ®")
            
            with col_chart2:
                st.markdown("#### ğŸ¯ é¢˜å‹å‡†ç¡®ç‡")
                
                type_df = report_generator.prepare_type_accuracy_data(report)
                
                if type_df is not None and not type_df.empty:
                    chart = alt.Chart(type_df).mark_bar().encode(
                        x=alt.X('é¢˜å‹', axis=alt.Axis(labelAngle=0)),
                        y=alt.Y('å‡†ç¡®ç‡', title='å‡†ç¡®ç‡ (%)'),
                        tooltip=['é¢˜å‹', 'å‡†ç¡®ç‡']
                    ).interactive()
                    st.altair_chart(chart, use_container_width=True)
                else:
                    st.info("æš‚æ— æ•°æ®")
            
            st.divider()
            
            st.subheader("ğŸ¤– AI å­¦ä¹ åé¦ˆ")
            
            if 'ai_feedback' not in st.session_state or st.session_state.get('feedback_report_id') != id(report):
                with st.spinner("ğŸ§  AIæ­£åœ¨åˆ†æ..."):
                    try:
                        feedback = report_generator.generate_study_feedback(
                            tokenizer=st.session_state.llm_tokenizer,
                            model=st.session_state.llm_model,
                            device=st.session_state.device,
                            report_data=report
                        )
                        
                        st.session_state.ai_feedback = feedback
                        st.session_state.feedback_report_id = id(report)
                        
                    except Exception as e:
                        st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                        feedback = report_generator.generate_fallback_feedback(report)
                        st.session_state.ai_feedback = feedback
            else:
                feedback = st.session_state.ai_feedback
            
            suggested_questions = re.findall(r'["â€œ](.*?)[â€"]', feedback)
            
            parts = re.split(r'["â€œ].*?[â€"]', feedback)

            st.markdown(parts[0])

            if suggested_questions:
                for i, question in enumerate(suggested_questions):
                    # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„key
                    button_key = f"suggest_q_{i}"
                    
                    # åˆ›å»ºæŒ‰é’®ï¼Œç‚¹å‡»åæ‰§è¡Œè·³è½¬é€»è¾‘
                    if st.button(f"ğŸ¤– åŠ©æ•™ï¼š{question}", key=button_key, use_container_width=True):
                        st.session_state.queued_rag_question = question
                        st.success(f"å·²å°†é—®é¢˜å‘é€åˆ°AIåŠ©æ•™ï¼è¯·åˆ‡æ¢æ ‡ç­¾é¡µæŸ¥çœ‹ã€‚")
                        
                    # æ˜¾ç¤ºæŒ‰é’®åçš„æ–‡æœ¬éƒ¨åˆ†
                    if (i + 1) < len(parts):
                        st.markdown(parts[i+1])
            else:
                # å¦‚æœæ²¡æœ‰æå–åˆ°é—®é¢˜ï¼Œå°±æ˜¾ç¤ºå®Œæ•´çš„åé¦ˆ
                if len(parts) > 1:
                     st.markdown("".join(parts[1:]))
                     
            st.divider()
            
            col_action1, col_action2, col_action3 = st.columns(3)
            
            with col_action1:
                if st.button("ğŸ¤– å‰å¾€AIåŠ©æ•™", use_container_width=True):
                    st.info("ğŸ‘‰ è¯·åˆ‡æ¢åˆ°ã€ŒAIåŠ©æ•™ã€æ ‡ç­¾é¡µ")
            
            with col_action2:
                if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆ", use_container_width=True):
                    if 'ai_feedback' in st.session_state:
                        del st.session_state.ai_feedback
                    if 'feedback_report_id' in st.session_state:
                        del st.session_state.feedback_report_id
                    st.rerun()
            
            with col_action3:
                export_format = st.selectbox(
                    "å¯¼å‡ºæ ¼å¼",
                    options=["TXT", "PDF"],
                    key="export_format"
                )
                
                if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", use_container_width=True):
                    try:
                        feedback_text = st.session_state.get('ai_feedback', 'æœªç”Ÿæˆ')
                        
                        if export_format == "TXT":
                            text_report = report_generator.export_report_to_text(
                                report_data=report,
                                feedback=feedback_text
                            )
                            
                            st.download_button(
                                label="ğŸ’¾ ä¸‹è½½ (TXT)",
                                data=text_report,
                                file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                mime="text/plain",
                                use_container_width=True
                            )
                        
                        elif export_format == "PDF":
                            pdf_buffer = report_generator.export_report_to_pdf(
                                report_data=report,
                                feedback=feedback_text
                            )
                            
                            st.download_button(
                                label="ğŸ’¾ ä¸‹è½½ (PDF)",
                                data=pdf_buffer,
                                file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                                mime="application/pdf",
                                use_container_width=True
                            )
                    
                    except Exception as e:
                        st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
    
    # ==================== æ ‡ç­¾é¡µ4ï¼šAIåŠ©æ•™ (ã€æ›¿æ¢ã€‘å¢å¼ºç‰ˆ) ====================
    with tab_rag:
        st.header("ğŸ¤– AIæ™ºèƒ½åŠ©æ•™")
        
        # ã€ä¿®æ”¹ã€‘æ£€æŸ¥é—®ç­”çŸ¥è¯†åº“
        if st.session_state.rag_retriever is None:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æˆ–åŠ è½½æ•™æ")
        else:
            # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„çŸ¥è¯†åº“ç±»å‹
            if st.session_state.session_db_path == STATIC_VECTOR_DB_PATH:
                st.info("ğŸ’¡ åŸºäº **é»˜è®¤æ•™æ** å›ç­”é—®é¢˜")
            else:
                st.success("ğŸ’¡ åŸºäº **é»˜è®¤æ•™æ + æ‚¨ä¸Šä¼ çš„PDFï¼ˆæ··åˆçŸ¥è¯†åº“ï¼‰** å›ç­”é—®é¢˜")
            
            # ã€æ–°å¢ã€‘æ£€æŸ¥æ˜¯å¦æœ‰æ¥è‡ªæŠ¥å‘Šé¡µçš„æ’é˜Ÿé—®é¢˜
            if st.session_state.get("queued_rag_question"):
                # è·å–é—®é¢˜å¹¶ç«‹å³æ¸…é™¤é˜Ÿåˆ—
                user_question = st.session_state.queued_rag_question
                del st.session_state.queued_rag_question
                
                # ã€å…³é”®ã€‘å°†è¿™ä¸ªé—®é¢˜æ¨¡æ‹Ÿä¸ºç”¨æˆ·åˆšåˆšçš„è¾“å…¥
                st.session_state.rag_messages.append({"role": "user", "content": user_question})
                
                # ç«‹å³æ‰§è¡Œä¸€æ¬¡RAGæµç¨‹ (å¤åˆ¶ç²˜è´´ä¸‹æ–¹çš„æµå¼é€»è¾‘)
                with st.chat_message("user"):
                    st.markdown(user_question)
                
                with st.chat_message("assistant"):
                    status_container = st.empty()
                    with status_container.status("ğŸ” æ­£åœ¨æ£€ç´¢...", expanded=False):
                        context, sources, docs = retrieve_with_enhancements(
                            st.session_state.rag_retriever,
                            user_question,
                            k=k_documents,
                            enable_expansion=enable_query_expansion
                        )
                    
                    if not docs:
                        full_response = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚"
                        st.markdown(full_response)
                        st.session_state.rag_messages.append({
                            "role": "assistant", "content": full_response, "sources": [], "question": user_question
                        })
                    else:
                        dialogue_history = None
                        if enable_multi_turn:
                            with status_container.status("ğŸ’­ åˆ†æå¯¹è¯...", expanded=False):
                                dialogue_history = extract_dialogue_context(
                                    st.session_state.rag_messages[:-1],
                                    max_history=max_history_turns
                                )
                        
                        with status_container.status("âœï¸ æ­£åœ¨ç”Ÿæˆ...", expanded=False):
                            messages = build_enhanced_prompt(
                                context, user_question, dialogue_history, use_fewshot, enable_multi_turn
                            )
                            response_placeholder = st.empty()
                            full_response = ""
                            try:
                                for chunk in generate_response_stream(
                                    st.session_state.llm_tokenizer, st.session_state.llm_model, st.session_state.device, messages
                                ):
                                    full_response += chunk
                                    response_placeholder.markdown(full_response + "â–Œ")
                                response_placeholder.markdown(full_response)
                                status_container.empty()
                            except Exception as e:
                                st.error(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
                                full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆæ—¶é‡åˆ°é—®é¢˜ã€‚"
                                response_placeholder.markdown(full_response)
                        
                        st.session_state.rag_messages.append({
                            "role": "assistant", "content": full_response, "sources": sources, "question": user_question
                        })
                        st.rerun() # ç«‹å³é‡è·‘ä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯å’Œåé¦ˆæŒ‰é’®

            # æ˜¾ç¤ºèŠå¤©å†å²
            for i, message in enumerate(st.session_state.rag_messages):
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
                    
                    # ä»…ä¸ºåŠ©æ•™æ¶ˆæ¯æ˜¾ç¤º æ¥æº å’Œ åé¦ˆ
                    if message["role"] == "assistant":
                        # å¼•ç”¨æ¥æº
                        if "sources" in message and message["sources"]:
                            with st.expander("ğŸ“š å¼•ç”¨æ¥æº"):
                                for j, source in enumerate(message["sources"], 1):
                                    st.text(f"{j}. {source}")
                        
                        # åé¦ˆæŒ‰é’® (ä½¿ç”¨å”¯ä¸€çš„key)
                        st.caption("åé¦ˆ")
                        col_like, col_dislike, _ = st.columns([1, 1, 8])
                        
                        with col_like:
                            if st.button("ğŸ‘", key=f"like_{i}"):
                                save_feedback(
                                    message.get("question", ""), # è·å–å¯¹åº”çš„é—®é¢˜
                                    message["content"],
                                    "helpful"
                                )
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
                        
                        with col_dislike:
                            if st.button("ğŸ‘", key=f"dislike_{i}"):
                                save_feedback(
                                    message.get("question", ""), # è·å–å¯¹åº”çš„é—®é¢˜
                                    message["content"],
                                    "unhelpful"
                                )
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
            
            # ç”¨æˆ·è¾“å…¥
            if user_question := st.chat_input("ğŸ’­ è¯·è¾“å…¥é—®é¢˜..."):
                st.session_state.rag_messages.append({
                    "role": "user",
                    "content": user_question
                })
                
                with st.chat_message("user"):
                    st.markdown(user_question)
                
                # å¼€å§‹æµå¼å›ç­”
                with st.chat_message("assistant"):
                    status_container = st.empty() # ç”¨äºæ˜¾ç¤ºçŠ¶æ€
                    
                    # 1. æ£€ç´¢
                    with status_container.status("ğŸ” æ­£åœ¨æ£€ç´¢...", expanded=False):
                        context, sources, docs = retrieve_with_enhancements(
                            st.session_state.rag_retriever,
                            user_question,
                            k=k_documents,
                            enable_expansion=enable_query_expansion
                        )
                    
                    if not docs:
                        full_response = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚"
                        st.markdown(full_response)
                        
                        # ä¿å­˜æ— ç­”æ¡ˆçš„å›ç­”
                        st.session_state.rag_messages.append({
                            "role": "assistant",
                            "content": full_response,
                            "sources": [],
                            "question": user_question
                        })
                    else:
                        # 2. æå–å¯¹è¯å†å²
                        dialogue_history = None
                        if enable_multi_turn:
                            with status_container.status("ğŸ’­ åˆ†æå¯¹è¯...", expanded=False):
                                dialogue_history = extract_dialogue_context(
                                    st.session_state.rag_messages[:-1], # æ’é™¤å½“å‰é—®é¢˜
                                    max_history=max_history_turns
                                )
                        
                        # 3. ç”Ÿæˆå›ç­”
                        with status_container.status("âœï¸ æ­£åœ¨ç”Ÿæˆ...", expanded=False):
                            messages = build_enhanced_prompt(
                                context,
                                user_question,
                                dialogue_history=dialogue_history,
                                use_fewshot=use_fewshot,
                                use_multi_turn=enable_multi_turn
                            )
                            
                            response_placeholder = st.empty()
                            full_response = ""
                            
                            try:
                                for chunk in generate_response_stream(
                                    st.session_state.llm_tokenizer,
                                    st.session_state.llm_model,
                                    st.session_state.device,
                                    messages
                                ):
                                    full_response += chunk
                                    response_placeholder.markdown(full_response + "â–Œ")
                                
                                response_placeholder.markdown(full_response) # æœ€ç»ˆæ˜¾ç¤º
                                status_container.empty() # æ¸…ç©ºçŠ¶æ€
                                
                            except Exception as e:
                                st.error(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
                                full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆæ—¶é‡åˆ°é—®é¢˜ã€‚"
                                response_placeholder.markdown(full_response)
                        
                        # 4. ä¿å­˜å®Œæ•´å›ç­”åˆ°å†å²
                        st.session_state.rag_messages.append({
                            "role": "assistant",
                            "content": full_response,
                            "sources": sources,
                            "question": user_question # ä¿å­˜å¯¹åº”çš„é—®é¢˜ï¼Œç”¨äºåé¦ˆ
                        })
                        
                        # 5. ç«‹å³é‡æ–°è¿è¡Œä»¥æ˜¾ç¤ºåé¦ˆæŒ‰é’®
                        st.rerun()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"åº”ç”¨è¿è¡Œå‡ºé”™: {e}")
        import traceback
        st.code(traceback.format_exc())