# quiz_module/report_generator.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Any
import pandas as pd
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import cm
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from io import BytesIO
import os

# ç”Ÿæˆé…ç½®
GENERATION_CONFIG = {
    "max_new_tokens": 1024,
    "temperature": 0.6,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.1,
}


@torch.no_grad()
def generate_study_feedback(
    tokenizer: AutoTokenizer, 
    model: AutoModelForCausalLM, 
    device: str, 
    report_data: Dict[str, Any]
) -> str:
    """
    ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ åé¦ˆ - ä¼˜åŒ–ç‰ˆ
    
    Args:
        tokenizer: åˆ†è¯å™¨
        model: è¯­è¨€æ¨¡å‹
        device: è®¾å¤‡
        report_data: æµ‹éªŒæŠ¥å‘Šæ•°æ®
    
    Returns:
        Markdownæ ¼å¼çš„å­¦ä¹ åé¦ˆ
    """
    
    # è·å–é”™é¢˜
    wrong_answers = [r for r in report_data['results'] if not r['is_correct']]
    
    # å…¨å¯¹æƒ…å†µ
    if not wrong_answers:
        return generate_perfect_score_feedback(report_data)
    
    # å‡†å¤‡é”™é¢˜ä¸Šä¸‹æ–‡
    context = _prepare_wrong_answers_context(wrong_answers, report_data)
    
    # æ„å»ºæç¤ºè¯ - ä¼˜åŒ–ç‰ˆ
    system_prompt = """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å­¦ä¹ é¡¾é—®ï¼Œæ“…é•¿åˆ†æå­¦ç”Ÿçš„æµ‹éªŒè¡¨ç°å¹¶æä¾›æœ‰é’ˆå¯¹æ€§çš„å­¦ä¹ å»ºè®®ã€‚

**è§’è‰²å®šä½ï¼š**
- ä¸“ä¸šä½†äº²åˆ‡ï¼Œåƒä¸€ä½å…³å¿ƒå­¦ç”Ÿæˆé•¿çš„å¯¼å¸ˆ
- å®¢è§‚åˆ†æé—®é¢˜ï¼Œä½†å§‹ç»ˆä¿æŒé¼“åŠ±å’Œå»ºè®¾æ€§çš„æ€åº¦
- æä¾›å…·ä½“å¯è¡Œçš„æ”¹è¿›æ–¹æ¡ˆï¼Œè€Œéç©ºæ´çš„å»ºè®®

**åé¦ˆåŸåˆ™ï¼š**
1. è‚¯å®šä¼˜åŠ¿ï¼ŒæŒ‡å‡ºä¸è¶³ï¼Œç»™äºˆæ–¹å‘
2. ä»é”™é¢˜ä¸­æç‚¼æ ¸å¿ƒé—®é¢˜ï¼ˆæ¦‚å¿µç†è§£ã€çŸ¥è¯†åº”ç”¨ç­‰ï¼‰
3. å»ºè®®å…·ä½“ã€å¯æ“ä½œï¼Œæœ‰æ˜ç¡®çš„å­¦ä¹ è·¯å¾„
4. è¯­è¨€ç®€æ´æ˜äº†ï¼Œé¿å…è¿‡åº¦ä¸“ä¸šæœ¯è¯­"""

    user_message = f"""{context}

**è¯·ç”Ÿæˆä¸€ä»½å­¦ä¹ åé¦ˆæŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š**

**1. æ•´ä½“è¯„ä»·** (2-3å¥è¯)
- æ€»ä½“è¡¨ç°å¦‚ä½•ï¼ˆå¾—åˆ†{report_data['score_percentage']:.1f}%ï¼‰
- ç­”å¯¹{report_data['correct']}/{report_data['total']}é¢˜çš„æ°´å¹³å®šä½
- ç®€è¦çš„é¼“åŠ±æˆ–è‚¯å®š

**2. çŸ¥è¯†ç›²åŒºåˆ†æ** (åˆ—å‡º2-3ä¸ªæ ¸å¿ƒé—®é¢˜)
- ä»é”™é¢˜ä¸­æç‚¼å‡ºçš„çŸ¥è¯†è–„å¼±ç‚¹
- æ¯ä¸ªç‚¹ç”¨ä¸€å¥è¯æ¦‚æ‹¬
- æŒ‰é‡è¦æ€§æ’åº

**3. é’ˆå¯¹æ€§å»ºè®®** (3-4æ¡å…·ä½“å»ºè®®)
- æ¯æ¡å»ºè®®é’ˆå¯¹ä¸€ä¸ªçŸ¥è¯†ç›²åŒº
- è¯´æ˜"åº”è¯¥åšä»€ä¹ˆ"å’Œ"å¦‚ä½•åš"
- å¯ä»¥æ¨èå…·ä½“çš„å­¦ä¹ æ–¹æ³•æˆ–èµ„æº

**4. ä¸‹ä¸€æ­¥è¡ŒåŠ¨**
- å¼•å¯¼å­¦ç”Ÿä½¿ç”¨"AIåŠ©æ•™"åŠŸèƒ½æ·±å…¥å­¦ä¹ é”™é¢˜
- ç»™å‡ºå…·ä½“çš„æé—®ç¤ºä¾‹

**æ ¼å¼è¦æ±‚ï¼š**
- ä½¿ç”¨Markdownæ ¼å¼
- ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨ç­‰ç»“æ„åŒ–å…ƒç´ 
- è¯­è¨€å‹å¥½ã€ä¸“ä¸šã€æœ‰æ¸©åº¦
- æ€»é•¿åº¦æ§åˆ¶åœ¨300-400å­—

ç›´æ¥è¾“å‡ºæŠ¥å‘Šå†…å®¹ï¼Œä¸è¦æœ‰é¢å¤–è¯´æ˜ã€‚"""

    messages = [
        {"role": "system", "content": system_prompt}, 
        {"role": "user", "content": user_message}
    ]
    
    try:
        text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        outputs = model.generate(**inputs, **GENERATION_CONFIG)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        return response
        
    except Exception as e:
        print(f"âš ï¸ AIåé¦ˆç”Ÿæˆå¤±è´¥: {e}")
        return generate_fallback_feedback(report_data)


def _prepare_wrong_answers_context(wrong_answers: List[Dict[str, Any]], report_data: Dict[str, Any]) -> str:
    """å‡†å¤‡é”™é¢˜åˆ†æä¸Šä¸‹æ–‡ - ä¼˜åŒ–ç‰ˆ"""
    
    context = f"""**å­¦ç”Ÿæµ‹éªŒæƒ…å†µï¼š**
- æ€»é¢˜æ•°: {report_data['total']}
- ç­”å¯¹: {report_data['correct']}
- ç­”é”™: {report_data['wrong']}
- å¾—åˆ†: {report_data['score_percentage']:.1f}%

**é”™é¢˜è¯¦æƒ…ï¼š**
"""
    
    for i, item in enumerate(wrong_answers, 1):
        context += f"\nã€é”™é¢˜ {i}ã€‘\n"
        context += f"é¢˜ç›®: {item['question']}\n"
        
        # è·å–ç­”æ¡ˆ
        try:
            user_ans_idx = item.get('user_answer', -1)
            if user_ans_idx == -1:
                user_ans_text = "æœªä½œç­”"
            else:
                user_ans_text = item['options'][user_ans_idx]
            
            correct_ans_text = item['options'][item['correct_answer']]
            
        except (IndexError, KeyError):
            user_ans_text = "æ— æ•ˆ"
            correct_ans_text = "æ— æ•ˆ"
        
        context += f"å­¦ç”Ÿç­”æ¡ˆ: {user_ans_text}\n"
        context += f"æ­£ç¡®ç­”æ¡ˆ: {correct_ans_text}\n"
        context += f"è§£æ: {item['explanation']}\n"
    
    return context


def generate_perfect_score_feedback(report_data: Dict[str, Any]) -> str:
    """å…¨å¯¹æ—¶çš„ç¥è´ºåé¦ˆ - ä¼˜åŒ–ç‰ˆ"""
    
    from .evaluator import get_performance_level
    performance = get_performance_level(report_data['score_percentage'])
    
    return f"""## {performance['emoji']} å®Œç¾è¡¨ç°ï¼

æ­å–œä½ å…¨éƒ¨ç­”å¯¹ï¼({report_data['correct']}/{report_data['total']}é¢˜)

### ğŸ“Š æˆç»©åˆ†æ
- **å¾—åˆ†**: {report_data['score_percentage']:.1f}%
- **è¯„çº§**: {performance['level']}
- {performance['message']}

### ğŸ’ª ä½ çš„ä¼˜åŠ¿
- **çŸ¥è¯†æ‰å®**: å¯¹æœ¬éƒ¨åˆ†å†…å®¹çš„æ ¸å¿ƒæ¦‚å¿µæŒæ¡ç‰¢å›º
- **ç†è§£æ·±å…¥**: èƒ½å¤Ÿå‡†ç¡®åŒºåˆ†ç›¸ä¼¼æ¦‚å¿µï¼Œç†è§£ç»†å¾®å·®åˆ«
- **åº”ç”¨ç†Ÿç»ƒ**: å°†ç†è®ºçŸ¥è¯†åº”ç”¨åˆ°å…·ä½“é—®é¢˜æ—¶æ¸¸åˆƒæœ‰ä½™

### ğŸš€ è¿›é˜¶å»ºè®®
1. **æŒ‘æˆ˜æ›´é«˜éš¾åº¦**: å°è¯•"å›°éš¾"çº§åˆ«çš„æµ‹éªŒï¼Œæ‹“å±•çŸ¥è¯†è¾¹ç•Œ
2. **æ·±åŒ–ç†è§£**: åœ¨AIåŠ©æ•™ä¸­æ¢è®¨æ›´æ·±å±‚çš„åŸç†å’Œæ•°å­¦æ¨å¯¼
3. **å®è·µåº”ç”¨**: å°†å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®æˆ–æ¡ˆä¾‹ä¸­
4. **çŸ¥è¯†è¿ç§»**: æ€è€ƒè¿™äº›æ¦‚å¿µåœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨

### ğŸ¤– ç»§ç»­æ¢ç´¢
æ—¢ç„¶åŸºç¡€å·²ç»å¾ˆæ‰å®ï¼Œä¸å¦¨åœ¨ã€ŒAIåŠ©æ•™ã€ä¸­å°è¯•è¿™äº›é—®é¢˜ï¼š
- "èƒ½å¦ä»æ•°å­¦è§’åº¦æ·±å…¥è§£é‡Š...çš„åŸç†ï¼Ÿ"
- "...æ–¹æ³•åœ¨å®é™…é¡¹ç›®ä¸­æœ‰å“ªäº›æ³¨æ„äº‹é¡¹ï¼Ÿ"
- "å¯¹æ¯”...å’Œ...çš„åº•å±‚å®ç°æœ‰ä½•ä¸åŒï¼Ÿ"

ä¿æŒè¿™ä»½çƒ­æƒ…å’Œä¸“æ³¨ï¼Œç»§ç»­åŠ æ²¹ï¼ ğŸŒŸ"""


def generate_fallback_feedback(report_data: Dict[str, Any]) -> str:
    """é™çº§åé¦ˆï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶ï¼‰- ä¼˜åŒ–ç‰ˆ"""
    
    from .evaluator import get_performance_level
    performance = get_performance_level(report_data['score_percentage'])
    
    feedback = f"""## {performance['emoji']} æµ‹éªŒåé¦ˆ

### ğŸ“Š æ€»ä½“è¡¨ç°
- **å¾—åˆ†**: {report_data['score_percentage']:.1f}%
- **è¯„çº§**: {performance['level']}
- **æ­£ç¡®**: {report_data['correct']}/{report_data['total']} é¢˜
- **é”™è¯¯**: {report_data['wrong']} é¢˜

{performance['message']}

### ğŸ¯ çŸ¥è¯†ç›²åŒº
"""
    
    # åˆ†æé”™é¢˜ç±»å‹
    wrong_answers = [r for r in report_data['results'] if not r['is_correct']]
    
    if wrong_answers:
        # ç®€å•åˆ†æï¼ˆåŸºäºé¢˜ç›®å…³é”®è¯ï¼‰
        knowledge_areas = set()
        for item in wrong_answers[:3]:
            question = item['question']
            # æå–å¯èƒ½çš„çŸ¥è¯†ç‚¹å…³é”®è¯
            if 'ç®—æ³•' in question or 'æ–¹æ³•' in question:
                knowledge_areas.add("ç®—æ³•å’Œæ–¹æ³•çš„ç†è§£")
            if 'åŸç†' in question or 'ä¸ºä»€ä¹ˆ' in question:
                knowledge_areas.add("åŸºç¡€åŸç†çš„æŒæ¡")
            if 'åº”ç”¨' in question or 'åœºæ™¯' in question:
                knowledge_areas.add("çŸ¥è¯†çš„å®é™…åº”ç”¨")
            if 'å¯¹æ¯”' in question or 'åŒºåˆ«' in question:
                knowledge_areas.add("æ¦‚å¿µçš„è¾¨æèƒ½åŠ›")
        
        if knowledge_areas:
            for area in list(knowledge_areas)[:3]:
                feedback += f"- {area}\n"
        else:
            feedback += "- è¯·ä»”ç»†å¤ä¹ é”™é¢˜æ¶‰åŠçš„çŸ¥è¯†ç‚¹\n"
    else:
        feedback += "- è¡¨ç°ä¼˜ç§€ï¼Œæ— æ˜æ˜¾çŸ¥è¯†ç›²åŒº\n"
    
    feedback += """
### ğŸ’¡ å­¦ä¹ å»ºè®®

**å¤ä¹ ç­–ç•¥ï¼š**
1. **ç²¾è¯»é”™é¢˜è§£æ**: ä¸è¦åªçœ‹ç­”æ¡ˆï¼Œç†è§£ä¸ºä»€ä¹ˆè¿™æ ·åš
2. **è¿½æ ¹æº¯æº**: å›åˆ°æ•™æï¼Œæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹å®Œæ•´å­¦ä¹ 
3. **ä¸¾ä¸€åä¸‰**: æ€è€ƒç±»ä¼¼çš„é—®é¢˜åº”è¯¥å¦‚ä½•è§£å†³

**æå‡æ–¹æ³•ï¼š**
1. **ä½¿ç”¨AIåŠ©æ•™**: é’ˆå¯¹ä¸ç†è§£çš„é”™é¢˜ï¼Œåœ¨åŠ©æ•™ä¸­æ·±å…¥æé—®
2. **ä¸»åŠ¨ç»ƒä¹ **: å®Œæˆç›¸å…³ä¹ é¢˜ï¼Œå·©å›ºè–„å¼±çŸ¥è¯†ç‚¹
3. **å®šæœŸå›é¡¾**: è¿‡å‡ å¤©å†æ¬¡æµ‹éªŒï¼Œæ£€éªŒå­¦ä¹ æ•ˆæœ

### ğŸ¤– æ¨èè¡ŒåŠ¨

ğŸ‘‰ ç«‹å³å‰å¾€ã€ŒAIåŠ©æ•™ã€ï¼Œå°è¯•è¿™æ ·æé—®ï¼š
- "è¯·è¯¦ç»†è§£é‡Š[é”™é¢˜ä¸­çš„æ¦‚å¿µ]"
- "ä¸ºä»€ä¹ˆ[é”™è¯¯é€‰é¡¹]ä¸æ­£ç¡®ï¼Ÿ"
- "èƒ½ä¸¾ä¸ª[çŸ¥è¯†ç‚¹]çš„å®é™…åº”ç”¨ä¾‹å­å—ï¼Ÿ"

æ¯æ¬¡æµ‹éªŒéƒ½æ˜¯è¿›æ­¥çš„æœºä¼šï¼Œç»§ç»­åŠªåŠ›ï¼ ğŸ’ª
"""
    
    return feedback


def prepare_chart_data(report_data: Dict[str, Any]) -> pd.DataFrame:
    """ä¸ºå¯è§†åŒ–å‡†å¤‡ç­”é¢˜åˆ†å¸ƒæ•°æ®"""
    data = {
        "ç±»åˆ«": ["âœ… ç­”å¯¹", "âŒ ç­”é”™"],
        "æ•°é‡": [
            report_data['correct'], 
            report_data['wrong']
        ]
    }
    
    if report_data.get('unanswered', 0) > 0:
        data["ç±»åˆ«"].append("â­• æœªç­”")
        data["æ•°é‡"].append(report_data['unanswered'])
    
    return pd.DataFrame(data)


def prepare_type_accuracy_data(report_data: Dict[str, Any]) -> pd.DataFrame:
    """å‡†å¤‡é¢˜å‹å‡†ç¡®ç‡æ•°æ®"""
    results = report_data.get('results', [])
    
    if not results:
        return None
    
    # ç»Ÿè®¡å„é¢˜å‹
    choice_correct = sum(1 for r in results if r.get('type') == 'choice' and r['is_correct'])
    choice_total = sum(1 for r in results if r.get('type') == 'choice')
    
    boolean_correct = sum(1 for r in results if r.get('type') == 'boolean' and r['is_correct'])
    boolean_total = sum(1 for r in results if r.get('type') == 'boolean')
    
    data = {
        "é¢˜å‹": [],
        "å‡†ç¡®ç‡": []
    }
    
    if choice_total > 0:
        data["é¢˜å‹"].append("ğŸ“‹ é€‰æ‹©é¢˜")
        data["å‡†ç¡®ç‡"].append(choice_correct / choice_total * 100)
    
    if boolean_total > 0:
        data["é¢˜å‹"].append("â“ åˆ¤æ–­é¢˜")
        data["å‡†ç¡®ç‡"].append(boolean_correct / boolean_total * 100)
    
    if not data["é¢˜å‹"]:
        return None
    
    return pd.DataFrame(data)


def export_report_to_text(report_data: Dict[str, Any], feedback: str) -> str:
    """å¯¼å‡ºæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
    
    lines = []
    lines.append("=" * 80)
    lines.append(" " * 28 + "ğŸ“Š å­¦ä¹ æµ‹éªŒæŠ¥å‘Š")
    lines.append("=" * 80)
    lines.append(f"\nğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
    
    # æˆç»©æ¦‚è§ˆ
    lines.append("â”Œ" + "â”€" * 78 + "â”")
    lines.append("â”‚" + " " * 30 + "æˆç»©æ¦‚è§ˆ" + " " * 40 + "â”‚")
    lines.append("â”œ" + "â”€" * 78 + "â”¤")
    lines.append(f"â”‚  æ€»é¢˜æ•°: {report_data['total']:<3}  æ­£ç¡®: {report_data['correct']:<3}  é”™è¯¯: {report_data['wrong']:<3}  å¾—åˆ†: {report_data['score_percentage']:.1f}%" + " " * 20 + "â”‚")
    
    from .evaluator import get_performance_level
    performance = get_performance_level(report_data['score_percentage'])
    lines.append(f"â”‚  è¯„çº§: {performance['level']}" + " " * 60 + "â”‚")
    lines.append("â””" + "â”€" * 78 + "â”˜")
    lines.append("")
    
    # AIå­¦ä¹ åé¦ˆ
    lines.append("=" * 80)
    lines.append(" " * 28 + "ğŸ¤– AI å­¦ä¹ åé¦ˆ")
    lines.append("=" * 80)
    lines.append(feedback)
    lines.append("")
    
    # è¯¦ç»†é¢˜ç›®
    lines.append("=" * 80)
    lines.append(" " * 30 + "ğŸ“‹ ç­”é¢˜è¯¦æƒ…")
    lines.append("=" * 80)
    
    for result in report_data['results']:
        idx = result['question_index']
        lines.append(f"\n{'â”' * 80}")
        lines.append(f"ç¬¬ {idx + 1} é¢˜")
        lines.append(f"é¢˜ç›®: {result['question']}")
        lines.append("\né€‰é¡¹:")
        
        for opt in result['options']:
            lines.append(f"  {opt}")
        
        correct_idx = result['correct_answer']
        user_idx = result.get('user_answer', -1)
        
        lines.append("")
        if result.get('is_unanswered', False):
            lines.append(f"æ‚¨çš„ç­”æ¡ˆ: â­• æœªä½œç­”")
            lines.append(f"æ­£ç¡®ç­”æ¡ˆ: {result['options'][correct_idx]}")
        elif result['is_correct']:
            lines.append(f"æ‚¨çš„ç­”æ¡ˆ: {result['options'][user_idx]} âœ…")
        else:
            lines.append(f"æ‚¨çš„ç­”æ¡ˆ: {result['options'][user_idx]} âŒ")
            lines.append(f"æ­£ç¡®ç­”æ¡ˆ: {result['options'][correct_idx]}")
        
        lines.append(f"\nè§£æ: {result['explanation']}")
    
    lines.append("\n" + "=" * 80)
    lines.append(" " * 32 + "æŠ¥å‘Šç»“æŸ")
    lines.append("=" * 80)
    
    return "\n".join(lines)


def export_report_to_pdf(report_data: Dict[str, Any], feedback: str) -> BytesIO:
    """
    å¯¼å‡ºPDFæ ¼å¼æŠ¥å‘Š - å­—ä½“ä¼˜åŒ–ç‰ˆ
    
    Args:
        report_data: æµ‹éªŒæŠ¥å‘Šæ•°æ®
        feedback: AIå­¦ä¹ åé¦ˆ
    
    Returns:
        BytesIO: PDFæ–‡ä»¶å­—èŠ‚æµ
    """
    
    buffer = BytesIO()
    
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=2*cm,
        leftMargin=2*cm,
        topMargin=2*cm,
        bottomMargin=2*cm
    )
    
    story = []
    styles = getSampleStyleSheet()
    
    # æ™ºèƒ½ä¸­æ–‡å­—ä½“æ£€æµ‹å’Œæ³¨å†Œ
    chinese_font_registered = False
    font_paths = [
        # macOS
        '/System/Library/Fonts/STHeiti Medium.ttc',
        '/System/Library/Fonts/PingFang.ttc',
        '/Library/Fonts/Arial Unicode.ttf',
        # Windows
        'C:/Windows/Fonts/simhei.ttf',
        'C:/Windows/Fonts/simsun.ttc',
        'C:/Windows/Fonts/msyh.ttc',
        # Linux
        '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',
        '/usr/share/fonts/truetype/arphic/uming.ttc',
        '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
    ]
    
    for font_path in font_paths:
        try:
            if os.path.exists(font_path):
                pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                chinese_font_registered = True
                break
        except Exception as e:
            continue
    
    # æ ¹æ®å­—ä½“å¯ç”¨æ€§è®¾ç½®æ ·å¼
    if chinese_font_registered:
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontName='ChineseFont',
            fontSize=24,
            textColor=colors.HexColor('#1f77b4'),
            spaceAfter=30,
            alignment=1
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName='ChineseFont',
            fontSize=16,
            textColor=colors.HexColor('#2ca02c'),
            spaceAfter=12
        )
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName='ChineseFont',
            fontSize=10,
            leading=14
        )
    else:
        # é™çº§åˆ°é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡ï¼‰
        import streamlit as st
        st.warning("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼ŒPDFä¸­çš„ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†")
        title_style = styles['Heading1']
        heading_style = styles['Heading2']
        normal_style = styles['Normal']
    
    # æ ‡é¢˜
    title = Paragraph("å­¦ä¹ æµ‹éªŒæŠ¥å‘Š", title_style)
    story.append(title)
    story.append(Spacer(1, 0.5*cm))
    
    # ç”Ÿæˆæ—¶é—´
    timestamp = Paragraph(
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
        normal_style
    )
    story.append(timestamp)
    story.append(Spacer(1, 0.5*cm))
    
    # æˆç»©æ¦‚è§ˆ
    story.append(Paragraph("æˆç»©æ¦‚è§ˆ", heading_style))
    
    from .evaluator import get_performance_level
    performance = get_performance_level(report_data['score_percentage'])
    
    summary_data = [
        ['é¡¹ç›®', 'æ•°å€¼'],
        ['æ€»é¢˜æ•°', str(report_data['total'])],
        ['æ­£ç¡®', str(report_data['correct'])],
        ['é”™è¯¯', str(report_data['wrong'])],
        ['å¾—åˆ†', f"{report_data['score_percentage']:.1f}%"],
        ['è¯„çº§', performance['level']]
    ]
    
    summary_table = Table(summary_data, colWidths=[8*cm, 8*cm])
    summary_table.setStyle(TableStyle([
        # 1. ã€ä¿®å¤ã€‘ä¸ºæ•´ä¸ªè¡¨æ ¼è®¾ç½®ä¸­æ–‡å­—ä½“
        ('FONTNAME', (0, 0), (-1, -1), 'ChineseFont'),
        
        # 2. è¡¨å¤´æ ·å¼
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTSIZE', (0, 0), (-1, 0), 12),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        
        # 3. å†…å®¹æ ·å¼
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        
        # 4. ç½‘æ ¼çº¿
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    
    story.append(summary_table)
    story.append(Spacer(1, 1*cm))
    
    # AIå­¦ä¹ åé¦ˆ
    story.append(Paragraph("AI å­¦ä¹ åé¦ˆ", heading_style))
    
    feedback_lines = feedback.split('\n')
    for line in feedback_lines:
        if line.strip():
            story.append(Paragraph(line, normal_style))
            story.append(Spacer(1, 0.2*cm))
    
    story.append(Spacer(1, 1*cm))
    
    # ç­”é¢˜è¯¦æƒ…
    story.append(Paragraph("ç­”é¢˜è¯¦æƒ…", heading_style))
    
    for result in report_data['results']:
        idx = result['question_index']
        
        q_title = Paragraph(f"ç¬¬ {idx + 1} é¢˜", heading_style)
        story.append(q_title)
        
        question_text = Paragraph(f"<b>é¢˜ç›®:</b> {result['question']}", normal_style)
        story.append(question_text)
        story.append(Spacer(1, 0.3*cm))
        
        for opt in result['options']:
            story.append(Paragraph(f"  {opt}", normal_style))
        
        story.append(Spacer(1, 0.3*cm))
        
        correct_idx = result['correct_answer']
        user_idx = result.get('user_answer', -1)
        
        if result.get('is_unanswered', False):
            story.append(Paragraph("æ‚¨çš„ç­”æ¡ˆ: æœªä½œç­”", normal_style))
            story.append(Paragraph(f"æ­£ç¡®ç­”æ¡ˆ: {result['options'][correct_idx]}", normal_style))
            status = Paragraph("<font color='orange'>â­• æœªä½œç­”</font>", normal_style)
        elif result['is_correct']:
            story.append(Paragraph(f"æ‚¨çš„ç­”æ¡ˆ: {result['options'][user_idx]}", normal_style))
            status = Paragraph("<font color='green'>âœ… æ­£ç¡®</font>", normal_style)
        else:
            story.append(Paragraph(f"æ‚¨çš„ç­”æ¡ˆ: {result['options'][user_idx]}", normal_style))
            story.append(Paragraph(f"æ­£ç¡®ç­”æ¡ˆ: {result['options'][correct_idx]}", normal_style))
            status = Paragraph("<font color='red'>âŒ é”™è¯¯</font>", normal_style)
        
        story.append(status)
        story.append(Spacer(1, 0.3*cm))
        
        story.append(Paragraph(f"<b>è§£æ:</b> {result['explanation']}", normal_style))
        story.append(Spacer(1, 0.8*cm))
    
    doc.build(story)
    
    buffer.seek(0)
    return buffer